<!doctype html><html><head><meta charset=UTF-8><title>Learn from Terminal Bench | Relaxed Yourself</title>
<link rel=stylesheet href=/blog/css/style.css><link rel=icon href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>ðŸ’¡</text></svg>"><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("search-input");e.addEventListener("input",function(){const e=this.value.toLowerCase(),t=document.querySelectorAll("main ul li a");t.forEach(t=>{const n=t.textContent.toLowerCase();n.includes(e)?t.parentElement.style.display="":t.parentElement.style.display="none"})})})</script></head><body><header><nav style=margin-bottom:1em><div class=nav-inner>| <a href=/blog/>Home</a>
| <a href=/blog/about/>About</a>
| <a href=/blog/tags/>Tags</a>
| <a href=/blog/categories/>Categories</a><form id=search-form onsubmit=return!1 style=display:inline-block;float:right><input type=text id=search-input placeholder=Filter... style="padding:2px 6px;font-size:.9rem"></form></div></nav><hr></header><main><h1>Learn from Terminal Bench</h1><div><p>After several days of learning from <a href=https://github.com/laude-institute/terminal-bench/><strong>terminal bench</strong></a>, I think that I need to note and summary what I learnt from the project.</p><h2 id=what-is-terminal-bench>What is Terminal Bench</h2><p>Terminal-Bench is the benchmark for testing AI agents in real terminal environments. From compiling code to training models and setting up servers, Terminal-Bench evaluates how well agents can handle real-world, end-to-end tasks - autonomously.</p><p><strong>How it work:</strong> The teriminal bench repo is a falsework, it build a simple terminal agent framework, <strong>Terminus</strong>, can process with tmux session in a docker container. It also use liteLLM to manage all LLM server for unified evaluation. After the falsework have built, a task contributer only need to create a task follow the task template. It include 4 main needs:</p><ol><li><em>task.yaml:</em> task description (prompt);</li><li><em>docker-compose.yaml & Dockerfile:</em> docker enviorment setup for run time toolkit preparation;</li><li><em>solution.sh:</em> oracle solution (make sure the provided task can be solved in terminal);</li><li><em>run-tests.sh & tests/test_outputs.py:</em> pytest script to verify the solution result.</li></ol><h2 id=what-i-have-got>What I Have Got</h2><ul><li><p>AI agents already can do so many hard tasks in programming (this amazing oberservation may change AI future), emm~ Certainty can be verified by pytest tasks.</p></li><li><p><strong>uv</strong> is a new tool to manage python packages, better and faster then pip.</p></li><li><p>Github action is a good PR and CI tool. ðŸ˜˜ (Using yaml and simple shell, you can manage what to do after you push to github )</p><ul><li>Auto code reviewer assignment</li><li>Auto code check (format <strong>ruff</strong> and quality checker <strong>claude review</strong>)</li><li>LLM code review (claude and gemini)</li><li>Auto multi agent process new task add to the bench. (yaml call from a script in sub-foler <em>scripts_bach/test-modified-tasks.sh</em> or <em>scripts_python</em>)</li></ul></li><li><p>Code agent can do code review, with the PR and CI pipline in github action, this unleashes a lot of automated coding potential.</p></li><li><p>Terminal Bench build an agent called <strong>Terminus</strong>. All the task can prcessed in docker container and process sub-feedback can get from a python package of a simulation tmux session. That&rsquo;s a good sandbox for safety LLM-Agent evaluation.</p></li><li><p>Build a pipline/workflow and everyone can collaborate more easily.</p></li></ul></div></main></body><footer><p>Â© 2025 sigabeauty | All rights reserved.</p></footer></html>