<!doctype html><html><head><meta charset=UTF-8><title>Review of Voice Activity Detection | Relaxed Yourself</title>
<link rel=stylesheet href=/blog/css/style.css><link rel=icon href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>ðŸ’¡</text></svg>"><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("search-input");e.addEventListener("input",function(){const e=this.value.toLowerCase(),t=document.querySelectorAll("main ul li a");t.forEach(t=>{const n=t.textContent.toLowerCase();n.includes(e)?t.parentElement.style.display="":t.parentElement.style.display="none"})})})</script></head><body><header><nav style=margin-bottom:1em><a href=/>Home</a> |
<a href=/about/>About</a> |
<a href=/tags/>Tags</a> |
<a href=/categories/>Categories</a><form id=search-form onsubmit=return!1 style=display:inline-block;float:right><input type=text id=search-input placeholder=Filter... style="padding:2px 6px;font-size:.9rem"></form></nav><hr></header><main><h1>Review of Voice Activity Detection</h1><div><p>In speech signal processing, auto speech recognition (ASR) is much well known than voice activity detection (VAD), because VAD a sub modual but not a full-application directly facing user interface. Indeed, VAD is very important in any production of speech data pre-processing.</p><h2 id=task-definition>Task Definition</h2><p>Voice activity detection is the task of looking for voice activity in an audio stream. Basically, it should tell speech apart from noise and silence. Given input audio feature X, the task of VAD is the predict Y, each Y_i is the probability of speech or noise/silence in current time. Here, the definition of X and Y is generalized in terms of time granularity(Can represent both audio chunk, frame, etc).</p><p>Based on the definition. Oh! This task seems easy enough, just a binary classification problem. But, a modern VAD should satisfy some main criteria</p><ul><li>High quality: Enough high precision and recall, maybe separately consider in different situations.</li><li>Low user perceived latency: Itâ€™s only a pre-processing step, important but cannot affect the whole pipeline latency.</li><li>Generalization: Robustness for all domains, audio sources, noise, quality, and SNR levels</li></ul><h2 id=related-works>Related Works</h2><h3 id=traditional-solution>Traditional Solution</h3><p>This kind of solution does not use any kind of external models, it uses signal processing methods or simple pattern recognition methods to estimate the silence and speech parts. Typically use GMM to model speech/silence frames and do iteratively estimation using algorithms like EM. This kind of solution has low latency but cannot generalize well for many situations. Indeed, it needs much parameter settings for a target situation to get usable performance. Googleâ€™s WebRTC VAD is a kind of this method.</p><h3 id=deep-solution>Deep Solution</h3><p>Deep neural network (DNN) is currently widely used in many signal processing tasks. DNN-based methods all need supervised label data for training. VAD is such a hard task for human annotation, we donâ€™t know what time granularity to labeling and human labeling can easily introduce errors. The time granularity of labeling decide the final system granularity, but as common sense, we know human may not hear clearly, and labeling is less than 100 ms.</p><p>Here are some typical DNN-based VAD.</p><p><strong>Pyannote</strong>: Pyannote is a toolkit for speaker diarization, VAD is a sub-modular in its pipeline. It use rttm-like file as a label file, randomly select a 2000 ms chunk and calculate the label for each frame in the chunk. With this processing method, it can also support speaker change point detection, overlap detection, end-to-end diarization.</p><p><strong>NeMo</strong>: Nemo is a NN toolkit developed by Nvidia that can use for many speech and NLP model implementations. The VAD detail in their published model MARBLENET, it use 630ms chunk size for a prediction. train, val and test chunk numbers are 160,994, 20,018 and 60,906. So, nearly 28 hours data.</p><p><strong>Silero</strong>: Silero VAD claimed itâ€™s trained using 13000h data, but it did not publish any training and data details, because they say their model is good enough and robust for any language in any situation. They do not need to publish their details, anyone did not need to train the model, only can use their model as a toolkit just like Googleâ€™s WebRTC VAD. In detail, in use a transformer-liked model. Using 512, 1024, 1536 samples for 16000 sample rate and 256, 512, 768 samples for 8000 sample rate. Nearly 30ms,60ms,100ms, itâ€™s almost frame-level.</p><p>PS: Some other methods use ASR model to perform force-alignment for the training data and get a frame-level label for VAD.</p><h2 id=others>Others</h2><p>Time granularity is important, it determines not only the performance but the availability of downstream tasks. Using deep solution, we need to face the problem of data collection and annotation. However, VAD seems a more or less solved task due to its simplicity and abundance of data in a way, but the reality is that we hardly use it in some situations.</p></div></main></body></html>